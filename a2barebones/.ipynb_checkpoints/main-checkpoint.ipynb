{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on train=5000 and test=5000 samples for run 0\n",
      "Running learner = BatchLinearRegression on parameters {'tol': 0.001, 'stepsize': 0.01, 'maxiter': 10000.0}\n",
      "Training took 70.82431149482727 s\n",
      "Error for BatchLinearRegression: 8.808703345054914\n",
      "Running on train=5000 and test=5000 samples for run 1\n",
      "Running learner = BatchLinearRegression on parameters {'tol': 0.001, 'stepsize': 0.01, 'maxiter': 10000.0}\n",
      "Training took 66.06569147109985 s\n",
      "Error for BatchLinearRegression: 8.846295358031698\n",
      "Running on train=5000 and test=5000 samples for run 2\n",
      "Running learner = BatchLinearRegression on parameters {'tol': 0.001, 'stepsize': 0.01, 'maxiter': 10000.0}\n",
      "Training took 74.46657609939575 s\n",
      "Error for BatchLinearRegression: 8.627090397360966\n",
      "Running on train=5000 and test=5000 samples for run 3\n",
      "Running learner = BatchLinearRegression on parameters {'tol': 0.001, 'stepsize': 0.01, 'maxiter': 10000.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8e92e231a5f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Running learner = '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlearnername\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' on parameters '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                 \u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training took {time.time() - start} s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;31m# Test model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\a2barebones\\regressionalgorithms.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, Xtrain, ytrain)\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_cost_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstepsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstepsize\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\a2barebones\\regressionalgorithms.py\u001b[0m in \u001b[0;36mgrad_cost_weights\u001b[1;34m(self, w, Xtrain, ytrain)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgrad_cost_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mline_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#script_regression.py\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import regressionalgorithms as algs\n",
    "\n",
    "import MLCourse.dataloader as dtl\n",
    "import MLCourse.plotfcns as plotfcns\n",
    "import time\n",
    "\n",
    "def l2err(prediction,ytest):\n",
    "    \"\"\" l2 error (i.e., root-mean-squared-error) \"\"\"\n",
    "    return np.linalg.norm(np.subtract(prediction, ytest))\n",
    "\n",
    "def l1err(prediction,ytest):\n",
    "    \"\"\" l1 error \"\"\"\n",
    "    return np.linalg.norm(np.subtract(prediction, ytest), ord=1)\n",
    "\n",
    "def l2err_squared(prediction,ytest):\n",
    "    \"\"\" l2 error squared \"\"\"\n",
    "    return np.square(np.linalg.norm(np.subtract(prediction, ytest)))\n",
    "\n",
    "def geterror(predictions, ytest):\n",
    "    # Can change this to other error values\n",
    "    return l2err(predictions, ytest) / np.sqrt(ytest.shape[0])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainsize = 5000\n",
    "    testsize = 5000\n",
    "    numruns = 5\n",
    "\n",
    "    regressionalgs = {\n",
    "        #'Random': algs.Regressor,\n",
    "        #'Mean': algs.MeanPredictor,\n",
    "        #'FSLinearRegression': algs.FSLinearRegression,\n",
    "        # 'RidgeLinearRegression': algs.RidgeLinearRegression,\n",
    "        # 'KernelLinearRegression': algs.KernelLinearRegression,\n",
    "        # 'LassoRegression': algs.LassoRegression,\n",
    "         'StochasticLinearRegression': algs.StochasticLinearRegression,\n",
    "        # 'MPLinearRegression': algs.MPLinearRegression,\n",
    "        # 'BatchLinearRegression': algs.BatchLinearRegression\n",
    "    }\n",
    "    numalgs = len(regressionalgs)\n",
    "\n",
    "    # Specify the name of the algorithm and an array of parameter values to try\n",
    "    # if an algorithm is not include, will run with default parameters\n",
    "    parameters = {\n",
    "        'FSLinearRegression': [\n",
    "            { 'features': [i for i in range(1, 20)] },\n",
    "            { 'features': [i for i in range(0, 385)] },\n",
    "        ],\n",
    "        'RidgeLinearRegression': [\n",
    "            { 'regwgt': 0.00 , 'features': [i for i in range(1, 20)]},\n",
    "            { 'regwgt': 0.01 , 'features': [i for i in range(1, 20)]},\n",
    "            { 'regwgt': 0.05 , 'features': [i for i in range(1, 20)]},\n",
    "        ],\n",
    "        'LassoRegression': [\n",
    "            { 'regwgt': 0.00 , 'features': [i for i in range(1, 20)], 'tol':10},\n",
    "            { 'regwgt': 0.01 , 'features': [i for i in range(1, 20)], 'tol':10},\n",
    "            { 'regwgt': 0.05 , 'features': [i for i in range(1, 20)], 'tol':10},\n",
    "        ],\n",
    "        'StochasticLinearRegression': [\n",
    "            { 'epochs': 1000,  'stepsize': 0.01},\n",
    "            { 'epochs': 3243,  'stepsize': 0.01}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    errors = {}\n",
    "    for learnername in regressionalgs:\n",
    "        # get the parameters to try for this learner\n",
    "        # if none specified, then default to an array of 1 parameter setting: None\n",
    "        params = parameters.get(learnername, [ None ])\n",
    "        errors[learnername] = np.zeros((len(params), numruns))\n",
    "\n",
    "    for r in range(numruns):\n",
    "        trainset, testset = dtl.load_ctscan(trainsize,testsize)\n",
    "        print(('Running on train={0} and test={1} samples for run {2}').format(trainset[0].shape[0], testset[0].shape[0], r))\n",
    "\n",
    "        for learnername, Learner in regressionalgs.items():\n",
    "            params = parameters.get(learnername, [ None ])\n",
    "            for p in range(len(params)):\n",
    "                start = time.time()\n",
    "                learner = Learner(params[p])\n",
    "                print ('Running learner = ' + learnername + ' on parameters ' + str(learner.getparams()))\n",
    "                # Train model\n",
    "                learner.learn(trainset[0], trainset[1])\n",
    "                print(f'Training took {time.time() - start} s')\n",
    "                # Test model\n",
    "                predictions = learner.predict(testset[0])\n",
    "                error = geterror(testset[1], predictions)\n",
    "                print ('Error for ' + learnername + ': ' + str(error))\n",
    "                errors[learnername][p, r] = error\n",
    "                \n",
    "                if learnername == 'BatchLinearRegression':\n",
    "                    batch_cost = learner.cost\n",
    "                    batch_time = learner.time\n",
    "\n",
    "    for learnername in regressionalgs:\n",
    "        params = parameters.get(learnername, [ None ])\n",
    "        besterror = np.mean(errors[learnername][0, :])\n",
    "        bestparams = 0\n",
    "        for p in range(len(params)):\n",
    "            aveerror = np.mean(errors[learnername][p, :])\n",
    "            if aveerror < besterror:\n",
    "                besterror = aveerror\n",
    "                bestparams = p\n",
    "\n",
    "        # Extract best parameters\n",
    "        best = params[bestparams]\n",
    "        print ('Best parameters for ' + learnername + ': ' + str(best))\n",
    "        print ('Average error for ' + learnername + ': ' + str(besterror) + ' +- ' + str(1.96 * np.std(errors[learnername][bestparams, :]) / math.sqrt(numruns)))\n",
    "        \n",
    "        sample_error = np.std(errors[learnername], axis=1)/np.sqrt(numruns)\n",
    "        print( f'Sample error for learner {learnername} is {sample_error}')\n",
    "        print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('figure',dpi=250)\n",
    "mpl.rc('text',usetex=True)\n",
    "\n",
    "plt.title('Batch Gradient Descent cost function by time')\n",
    "plt.xlabel('Runtime (s)')\n",
    "plt.ylabel('Cost function')\n",
    "plt.plot(batch_time, batch_cost)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.title('Batch Gradient Descent cost function by iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost function')\n",
    "plt.plot(np.arange(len(batch_time)), batch_cost)\n",
    "\n",
    "print(len(batch_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
